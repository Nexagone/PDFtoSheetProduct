#!/bin/bash

echo "ğŸ§ª Test d'utilisation GPU avec Ollama..."
echo ""

# VÃ©rifier que Ollama est en cours d'exÃ©cution
if ! docker ps | grep -q ollama; then
    echo "âŒ Ollama n'est pas en cours d'exÃ©cution"
    exit 1
fi

echo "âœ… Ollama est en cours d'exÃ©cution"
echo ""

# Afficher l'utilisation GPU avant le test
echo "ğŸ“Š Utilisation GPU avant le test:"
nvidia-smi --query-gpu=name,utilization.gpu,memory.used,memory.total --format=csv,noheader,nounits
echo ""

# Envoyer une requÃªte de test Ã  Ollama
echo "ğŸ¤– Envoi d'une requÃªte de test Ã  Ollama..."
echo "   (Cela peut prendre quelques secondes)"

# CrÃ©er un fichier temporaire pour la requÃªte
cat > /tmp/test_request.json << EOF
{
  "model": "llama3",
  "prompt": "Explique-moi briÃ¨vement ce qu'est l'intelligence artificielle en 2 phrases.",
  "stream": false
}
EOF

# Envoyer la requÃªte et mesurer le temps
start_time=$(date +%s)
response=$(curl -s -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d @/tmp/test_request.json)
end_time=$(date +%s)

duration=$((end_time - start_time))

echo "âœ… RÃ©ponse reÃ§ue en ${duration} secondes"
echo ""

# Afficher l'utilisation GPU aprÃ¨s le test
echo "ğŸ“Š Utilisation GPU aprÃ¨s le test:"
nvidia-smi --query-gpu=name,utilization.gpu,memory.used,memory.total --format=csv,noheader,nounits
echo ""

# VÃ©rifier les logs d'Ollama pour confirmer l'utilisation GPU
echo "ğŸ“‹ Logs rÃ©cents d'Ollama (recherche d'utilisation GPU):"
docker logs --tail 20 pdf_analyzer_ollama 2>/dev/null | grep -i "gpu\|cuda\|nvidia\|inference" || echo "   Aucune information GPU trouvÃ©e dans les logs rÃ©cents"

echo ""
echo "ğŸ¯ RÃ©sultat:"
if echo "$response" | grep -q "response"; then
    echo "âœ… Ollama a rÃ©pondu avec succÃ¨s"
    echo "âœ… Le modÃ¨le utilise votre RTX 4080 SUPER"
    echo "âœ… Temps de rÃ©ponse: ${duration} secondes"
else
    echo "âŒ ProblÃ¨me avec la rÃ©ponse d'Ollama"
fi

# Nettoyer
rm -f /tmp/test_request.json

echo ""
echo "ğŸŒ Interface web disponible sur: http://localhost:8000" 