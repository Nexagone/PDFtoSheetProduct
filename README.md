# GÃ©nÃ©rateur de Fiches Produit PDF â†’ HTML/PDF

Un service intelligent qui transforme automatiquement les PDF constructeur en fiches produit professionnelles en utilisant l'IA (Ollama/Llama).

## ğŸš€ FonctionnalitÃ©s

- **Analyse intelligente** : Extraction automatique des informations produit depuis les PDF constructeur
- **Interface moderne** : Interface web drag & drop intuitive
- **Sorties multiples** : GÃ©nÃ©ration de fiches produit en HTML et/ou PDF
- **Design professionnel** : Templates modernes et responsives
- **API REST** : Endpoints pour intÃ©gration avec d'autres systÃ¨mes
- **DockerisÃ©** : DÃ©ploiement facile avec Docker Compose

## ğŸ—ï¸ Architecture

```
PDFtoSheetProduct/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py              # Application FastAPI principale
â”‚   â”œâ”€â”€ config.py            # Configuration
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ pdf_analyzer.py  # Service d'analyse PDF avec Ollama
â”‚   â”‚   â”œâ”€â”€ html_generator.py # GÃ©nÃ©rateur HTML
â”‚   â”‚   â””â”€â”€ pdf_generator.py  # GÃ©nÃ©rateur PDF
â”‚   â””â”€â”€ templates/
â”‚       â”œâ”€â”€ index.html       # Interface utilisateur
â”‚       â””â”€â”€ product_sheet.html # Template fiche produit
â”œâ”€â”€ uploads/                 # Fichiers PDF uploadÃ©s
â”œâ”€â”€ outputs/                 # Fichiers gÃ©nÃ©rÃ©s
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ requirements.txt
```

## ğŸ› ï¸ Installation et DÃ©marrage

### PrÃ©requis

- Docker et Docker Compose
- Au moins 4GB de RAM disponible (pour Ollama)

### DÃ©marrage rapide

1. **Cloner le projet**
```bash
git clone <repository-url>
cd PDFtoSheetProduct
```

2. **Lancer avec Docker Compose**
```bash
docker-compose up -d
```

3. **AccÃ©der Ã  l'application**
- Interface web : http://localhost:8000
- API docs : http://localhost:8000/docs

### PremiÃ¨re utilisation

Lors du premier dÃ©marrage :
1. Ollama se lance automatiquement
2. Le modÃ¨le Llama3 se tÃ©lÃ©charge (peut prendre plusieurs minutes)
3. L'application attend qu'Ollama soit prÃªt avant de dÃ©marrer

## ğŸ“– Utilisation

### Interface Web

1. **Ouvrir** http://localhost:8000
2. **Glisser-dÃ©poser** un fichier PDF constructeur
3. **Choisir** le format de sortie (HTML, PDF, ou les deux)
4. **Cliquer** sur "GÃ©nÃ©rer la fiche produit"
5. **TÃ©lÃ©charger** les fichiers gÃ©nÃ©rÃ©s

### API REST

#### Upload et traitement
```bash
curl -X POST "http://localhost:8000/upload" \
  -H "accept: application/json" \
  -H "Content-Type: multipart/form-data" \
  -F "file=@document.pdf" \
  -F "output_format=both"
```

#### TÃ©lÃ©chargement
```bash
curl -X GET "http://localhost:8000/download/{session_id}/{filename}" \
  --output fichier_local.html
```

## ğŸ”§ Configuration

### Variables d'environnement

CrÃ©er un fichier `.env` pour personnaliser :

```env
# Configuration Ollama
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=llama3
OLLAMA_TIMEOUT=30

# Configuration des dossiers
UPLOAD_DIR=uploads
OUTPUT_DIR=outputs

# Configuration du serveur
HOST=0.0.0.0
PORT=8000

# Configuration des retries
MAX_RETRIES=3
RETRY_DELAY=2
```

### ModÃ¨les Ollama supportÃ©s

- `llama3` (recommandÃ©)
- `llama3.2`
- `mistral`
- `codellama`
- Tout autre modÃ¨le compatible Ollama

## ğŸ“Š Structure des donnÃ©es extraites

Le service extrait automatiquement :

```json
{
  "product_name": "Nom du produit",
  "brand": "Marque",
  "model_number": "RÃ©fÃ©rence modÃ¨le",
  "category": "CatÃ©gorie",
  "technical_specs": {
    "volume": "Volume",
    "classe_energetique": "Classe Ã©nergÃ©tique",
    "capacite": "CapacitÃ©",
    "puissance": "Puissance",
    "tension": "Tension",
    "frequence": "FrÃ©quence"
  },
  "dimensions": {
    "longueur": "Longueur",
    "largeur": "Largeur", 
    "hauteur": "Hauteur",
    "profondeur": "Profondeur"
  },
  "weight": "Poids",
  "power_consumption": "Consommation Ã©lectrique",
  "features": ["FonctionnalitÃ© 1", "FonctionnalitÃ© 2"],
  "warranty": "Garantie",
  "price_range": "Gamme de prix",
  "description": "Description dÃ©taillÃ©e",
  "color": "Couleur",
  "material": "MatÃ©riau",
  "certifications": ["Certification 1", "Certification 2"]
}
```

## ğŸ³ Commandes Docker utiles

### Voir les logs
```bash
# Logs de l'application
docker-compose logs -f app

# Logs d'Ollama
docker-compose logs -f ollama

# Tous les logs
docker-compose logs -f
```

### RedÃ©marrer un service
```bash
docker-compose restart app
```

### ArrÃªter complÃ¨tement
```bash
docker-compose down
```

### Nettoyer les volumes
```bash
docker-compose down -v
```

## ğŸ” DÃ©pannage

### Ollama ne dÃ©marre pas
```bash
# VÃ©rifier les logs
docker-compose logs ollama

# RedÃ©marrer le service
docker-compose restart ollama
```

### ModÃ¨le non trouvÃ©
```bash
# Se connecter au conteneur Ollama
docker exec -it pdf_analyzer_ollama bash

# Lister les modÃ¨les
ollama list

# TÃ©lÃ©charger un modÃ¨le
ollama pull llama3
```

### ProblÃ¨mes de mÃ©moire
- Augmenter la RAM allouÃ©e Ã  Docker
- Utiliser un modÃ¨le plus lÃ©ger (mistral au lieu de llama3)

## ğŸš€ DÃ©ploiement en production

### Avec Docker Compose
```bash
# Build en mode production
docker-compose -f docker-compose.prod.yml up -d
```

### Avec Kubernetes
```bash
kubectl apply -f k8s/
```

## ğŸ“ DÃ©veloppement

### Installation locale
```bash
# CrÃ©er un environnement virtuel
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate     # Windows

# Installer les dÃ©pendances
pip install -r requirements.txt

# Lancer Ollama localement
ollama serve

# Lancer l'application
uvicorn app.main:app --reload
```

### Tests
```bash
# Tests unitaires
pytest tests/

# Tests d'intÃ©gration
pytest tests/integration/
```

## ğŸ¤ Contribution

1. Fork le projet
2. CrÃ©er une branche feature (`git checkout -b feature/AmazingFeature`)
3. Commit les changements (`git commit -m 'Add some AmazingFeature'`)
4. Push vers la branche (`git push origin feature/AmazingFeature`)
5. Ouvrir une Pull Request

## ğŸ“„ Licence

Ce projet est sous licence MIT. Voir le fichier `LICENSE` pour plus de dÃ©tails.

## ğŸ†˜ Support

- **Issues** : [GitHub Issues](https://github.com/votre-repo/issues)
- **Documentation** : [Wiki](https://github.com/votre-repo/wiki)
- **Email** : support@votre-email.com

---

**Note** : Ce projet nÃ©cessite Ollama pour fonctionner. Assurez-vous d'avoir suffisamment de ressources systÃ¨me pour exÃ©cuter les modÃ¨les d'IA. 