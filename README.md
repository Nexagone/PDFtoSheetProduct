# GÃ©nÃ©rateur de Fiches Produit PDF â†’ HTML/PDF

Un service intelligent qui transforme automatiquement les PDF constructeur en fiches produit professionnelles en utilisant l'IA (Ollama/Llama).

## ğŸš€ FonctionnalitÃ©s

- **Analyse intelligente** : Extraction automatique des informations produit depuis les PDF constructeur
- **Interface moderne** : Interface web drag & drop intuitive
- **Sorties multiples** : GÃ©nÃ©ration de fiches produit en HTML et/ou PDF
- **Design professionnel** : Templates modernes et responsives
- **API REST** : Endpoints pour intÃ©gration avec d'autres systÃ¨mes
- **DockerisÃ©** : DÃ©ploiement facile avec Docker Compose

## ğŸ—ï¸ Architecture

```
PDFtoSheetProduct/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py              # Application FastAPI principale
â”‚   â”œâ”€â”€ config.py            # Configuration
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ pdf_analyzer.py  # Service d'analyse PDF avec Ollama
â”‚   â”‚   â”œâ”€â”€ html_generator.py # GÃ©nÃ©rateur HTML
â”‚   â”‚   â””â”€â”€ pdf_generator.py  # GÃ©nÃ©rateur PDF
â”‚   â””â”€â”€ templates/
â”‚       â”œâ”€â”€ index.html       # Interface utilisateur
â”‚       â””â”€â”€ product_sheet.html # Template fiche produit
â”œâ”€â”€ uploads/                 # Fichiers PDF uploadÃ©s
â”œâ”€â”€ outputs/                 # Fichiers gÃ©nÃ©rÃ©s
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ requirements.txt
```

## ğŸ› ï¸ Installation et DÃ©marrage

### PrÃ©requis

- Docker et Docker Compose
- Au moins 4GB de RAM disponible (pour Ollama)
- **Support GPU (optionnel)** : nvidia-container-toolkit pour accÃ©lÃ©ration GPU

### DÃ©marrage rapide

1. **Cloner le projet**
```bash
git clone <repository-url>
cd PDFtoSheetProduct
```

2. **Lancer avec Docker Compose**
```bash
docker-compose up -d
```

**Note** : Pour l'accÃ©lÃ©ration GPU, installez d'abord nvidia-container-toolkit :
```bash
sudo ./install-gpu-support.sh
```

3. **AccÃ©der Ã  l'application**
- Interface web : http://localhost:8000
- API docs : http://localhost:8000/docs

### PremiÃ¨re utilisation

Lors du premier dÃ©marrage :
1. Ollama se lance automatiquement
2. Le modÃ¨le Llama3 se tÃ©lÃ©charge (peut prendre plusieurs minutes)
3. L'application attend qu'Ollama soit prÃªt avant de dÃ©marrer

## ğŸ“– Utilisation

### Interface Web

1. **Ouvrir** http://localhost:8000
2. **Glisser-dÃ©poser** un fichier PDF constructeur
3. **Choisir** le format de sortie (HTML, PDF, ou les deux)
4. **Cliquer** sur "GÃ©nÃ©rer la fiche produit"
5. **TÃ©lÃ©charger** les fichiers gÃ©nÃ©rÃ©s

### API REST

#### Upload et traitement
```bash
curl -X POST "http://localhost:8000/upload" \
  -H "accept: application/json" \
  -H "Content-Type: multipart/form-data" \
  -F "file=@document.pdf" \
  -F "output_format=both"
```

#### TÃ©lÃ©chargement
```bash
curl -X GET "http://localhost:8000/download/{session_id}/{filename}" \
  --output fichier_local.html
```

## ğŸ”§ Configuration

### Variables d'environnement

CrÃ©er un fichier `.env` pour personnaliser :

```env
# Configuration Ollama
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=llama3
OLLAMA_TIMEOUT=30

# Configuration des dossiers
UPLOAD_DIR=uploads
OUTPUT_DIR=outputs

# Configuration du serveur
HOST=0.0.0.0
PORT=8000

# Configuration des retries
MAX_RETRIES=3
RETRY_DELAY=2
```

### ModÃ¨les Ollama supportÃ©s

- `llama3` (recommandÃ©)
- `llama3.2`
- `mistral`
- `codellama`
- Tout autre modÃ¨le compatible Ollama

## ğŸ“Š Structure des donnÃ©es extraites

Le service extrait automatiquement et sauvegarde automatiquement les rÃ©ponses du modÃ¨le dans le dossier `outputs/model_responses/` pour analyse et amÃ©lioration.

### DonnÃ©es extraites :

```json
{
  "product_name": "Nom du produit",
  "brand": "Marque",
  "model_number": "RÃ©fÃ©rence modÃ¨le",
  "category": "CatÃ©gorie",
  "technical_specs": {
    "volume": "Volume",
    "classe_energetique": "Classe Ã©nergÃ©tique",
    "capacite": "CapacitÃ©",
    "puissance": "Puissance",
    "tension": "Tension",
    "frequence": "FrÃ©quence"
  },
  "dimensions": {
    "longueur": "Longueur",
    "largeur": "Largeur", 
    "hauteur": "Hauteur",
    "profondeur": "Profondeur"
  },
  "weight": "Poids",
  "power_consumption": "Consommation Ã©lectrique",
  "features": ["FonctionnalitÃ© 1", "FonctionnalitÃ© 2"],
  "warranty": "Garantie",
  "price_range": "Gamme de prix",
  "description": "Description dÃ©taillÃ©e",
  "color": "Couleur",
  "material": "MatÃ©riau",
  "certifications": ["Certification 1", "Certification 2"]
}
```

### ğŸ“ Structure des fichiers sauvegardÃ©s

```
outputs/
â”œâ”€â”€ {session_id}/
â”‚   â”œâ”€â”€ model_responses/
â”‚   â”‚   â”œâ”€â”€ {session_id}_{timestamp}_model_response.json
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ product_sheet.html
â”‚   â””â”€â”€ product_sheet.pdf
```

### ğŸ” Analyse des rÃ©ponses du modÃ¨le

Pour analyser les rÃ©ponses sauvegardÃ©es :

```bash
# Visualiser toutes les rÃ©ponses
python view-model-responses.py

# Nettoyer les anciennes rÃ©ponses (plus de 7 jours)
python cleanup-model-responses.py clean 7

# Lister les rÃ©ponses par Ã¢ge
python cleanup-model-responses.py list

# Tester les rÃ©ponses en franÃ§ais
./test-french-response.sh

### ğŸ“‹ Contenu des fichiers de rÃ©ponse

Chaque fichier `model_response.json` contient :
- **MÃ©tadonnÃ©es** : Session ID, timestamp, modÃ¨le utilisÃ©
- **Prompt** : Le prompt envoyÃ© au modÃ¨le
- **RÃ©ponse brute** : La rÃ©ponse complÃ¨te d'Ollama
- **DonnÃ©es parsÃ©es** : Les donnÃ©es extraites et validÃ©es
- **Informations d'analyse** : Statistiques sur l'analyse

### ğŸ‡«ğŸ‡· Garantie de rÃ©ponse en franÃ§ais

Le systÃ¨me garantit que toutes les rÃ©ponses sont en franÃ§ais, mÃªme pour des documents en anglais :
- **Prompt renforcÃ©** : Instructions strictes pour rÃ©pondre en franÃ§ais
- **DÃ©tection automatique** : Alerte si du texte anglais est dÃ©tectÃ©
- **Validation** : VÃ©rification de la langue dans les donnÃ©es extraites
- **Test automatisÃ©** : Script de test pour vÃ©rifier les rÃ©ponses en franÃ§ais

## ğŸ³ Commandes Docker utiles

### Voir les logs
```bash
# Logs de l'application
docker-compose logs -f app

# Logs d'Ollama
docker-compose logs -f ollama

# Tous les logs
docker-compose logs -f
```

### RedÃ©marrer un service
```bash
docker-compose restart app
```

### ArrÃªter complÃ¨tement
```bash
docker-compose down
```

### Nettoyer les volumes
```bash
docker-compose down -v
```

## ğŸ” DÃ©pannage

### Ollama ne dÃ©marre pas
```bash
# VÃ©rifier les logs
docker-compose logs ollama

# RedÃ©marrer le service
docker-compose restart ollama
```

### ProblÃ¨mes GPU
```bash
# VÃ©rifier l'installation GPU
docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi

# RÃ©installer le support GPU
sudo ./install-gpu-support.sh

# DÃ©marrer Ollama manuellement
./start-ollama-manual.sh
```

### ModÃ¨le non trouvÃ©
```bash
# Se connecter au conteneur Ollama
docker exec -it pdf_analyzer_ollama bash

# Lister les modÃ¨les
ollama list

# TÃ©lÃ©charger un modÃ¨le
ollama pull llama3
```

### ProblÃ¨mes de mÃ©moire
- Augmenter la RAM allouÃ©e Ã  Docker
- Utiliser un modÃ¨le plus lÃ©ger (mistral au lieu de llama3)

## ğŸš€ DÃ©ploiement en production

### Avec Docker Compose
```bash
# Build en mode production
docker-compose -f docker-compose.prod.yml up -d
```

## ğŸ“ DÃ©veloppement

### Installation locale
```bash
# CrÃ©er un environnement virtuel
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate     # Windows

# Installer les dÃ©pendances
pip install -r requirements.txt

# Lancer Ollama localement
ollama serve

# Lancer l'application
uvicorn app.main:app --reload
```

### Tests
```bash
# Tests unitaires
pytest tests/

# Tests d'intÃ©gration
pytest tests/integration/
```

## ğŸ“„ Licence

Ce projet est sous licence MIT. Voir le fichier `LICENSE` pour plus de dÃ©tails.

---

**Note** : Ce projet nÃ©cessite Ollama pour fonctionner. Assurez-vous d'avoir suffisamment de ressources systÃ¨me pour exÃ©cuter les modÃ¨les d'IA. 